2022-11-16 21:43:07.699925: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
[34m[1mwandb[39m[22m: [33mWARNING[39m The save_model argument by default saves the model in the HDF5 format that cannot save custom objects like subclassed models and custom layers. This behavior will be deprecated in a future release in favor of the SavedModel format. Meanwhile, the HDF5 model is saved as W&B files and the SavedModel as W&B Artifacts.
Epoch 1/36

2264/2291 [============================>.] - ETA: 0s - loss: 344.2425

2264/2291 [============================>.] - ETA: 0s - loss: 344.2425INFO:tensorflow:Assets written to: /home/oscar47/Desktop/astro101/Astro101-Final-Project/nn_v0.0.1/wandb/run-20221116_214304-bow0gwu0/files/model-best/assets
2291/2291 [==============================] - 5s 2ms/step - loss: 340.3280 - val_loss: 0.8971
Epoch 2/36
2291/2291 [==============================] - 4s 2ms/step - loss: 0.9894 - val_loss: 0.9395
Epoch 3/36

2286/2291 [============================>.] - ETA: 0s - loss: 0.8400

2286/2291 [============================>.] - ETA: 0s - loss: 0.8400INFO:tensorflow:Assets written to: /home/oscar47/Desktop/astro101/Astro101-Final-Project/nn_v0.0.1/wandb/run-20221116_214304-bow0gwu0/files/model-best/assets
2291/2291 [==============================] - 4s 2ms/step - loss: 0.8400 - val_loss: 0.8845
Epoch 4/36
2264/2291 [============================>.] - ETA: 0s - loss: 0.8390

2264/2291 [============================>.] - ETA: 0s - loss: 0.8390INFO:tensorflow:Assets written to: /home/oscar47/Desktop/astro101/Astro101-Final-Project/nn_v0.0.1/wandb/run-20221116_214304-bow0gwu0/files/model-best/assets
2291/2291 [==============================] - 4s 2ms/step - loss: 0.8388 - val_loss: 0.8151
Epoch 5/36
2291/2291 [==============================] - 4s 2ms/step - loss: 59983824.0000 - val_loss: 84476.1484
Epoch 6/36
2291/2291 [==============================] - 4s 2ms/step - loss: 45608.3281 - val_loss: 23953.9375
Epoch 7/36

2291/2291 [==============================] - 4s 2ms/step - loss: 23368.1055 - val_loss: 14612.2637
Epoch 8/36

2291/2291 [==============================] - 4s 2ms/step - loss: 19081.2676 - val_loss: 836.7087
Epoch 9/36

2291/2291 [==============================] - 4s 2ms/step - loss: 1797.3979 - val_loss: 984.5734
Epoch 10/36

2291/2291 [==============================] - 4s 2ms/step - loss: 63105720.0000 - val_loss: 116576.6719
Epoch 11/36
2291/2291 [==============================] - 4s 2ms/step - loss: 77182.2734 - val_loss: 36753.9805
Epoch 12/36

2291/2291 [==============================] - 4s 2ms/step - loss: 174164032.0000 - val_loss: 1851021.6250
Epoch 13/36

2291/2291 [==============================] - 4s 2ms/step - loss: 983613.6875 - val_loss: 647502.0000
Epoch 14/36

2291/2291 [==============================] - 4s 2ms/step - loss: 263461.0312 - val_loss: 122981.1484
Epoch 15/36

2291/2291 [==============================] - 4s 2ms/step - loss: 92838.1406 - val_loss: 73981.1250
Epoch 16/36
2291/2291 [==============================] - 4s 2ms/step - loss: 178397664.0000 - val_loss: 607137.0625
Epoch 17/36

2291/2291 [==============================] - 4s 2ms/step - loss: 635510.8750 - val_loss: 270064.9375
Epoch 18/36

2291/2291 [==============================] - 4s 2ms/step - loss: 263051.3438 - val_loss: 266944.5625
Epoch 19/36

2291/2291 [==============================] - 4s 2ms/step - loss: 121463832.0000 - val_loss: 471133.4375
Epoch 20/36

2291/2291 [==============================] - 4s 2ms/step - loss: 381309.0000 - val_loss: 99689.8594
Epoch 21/36
2291/2291 [==============================] - 4s 2ms/step - loss: 365730272.0000 - val_loss: 6115125.0000
Epoch 22/36

2291/2291 [==============================] - 4s 2ms/step - loss: 3617395.7500 - val_loss: 1646993.7500
Epoch 23/36

2291/2291 [==============================] - 4s 2ms/step - loss: 73229656.0000 - val_loss: 86103824.0000
Epoch 24/36

2291/2291 [==============================] - 4s 2ms/step - loss: 2231092.5000 - val_loss: 325957.8125
Epoch 25/36

2291/2291 [==============================] - 4s 2ms/step - loss: 295652.0938 - val_loss: 207538.0938
Epoch 26/36

2291/2291 [==============================] - 4s 2ms/step - loss: 57096360.0000 - val_loss: 604851.6250
Epoch 27/36
2291/2291 [==============================] - 4s 2ms/step - loss: 349123.1875 - val_loss: 147459.5000
Epoch 28/36

2291/2291 [==============================] - 4s 2ms/step - loss: 137456.8125 - val_loss: 46492.7578
Epoch 29/36

2291/2291 [==============================] - 4s 2ms/step - loss: 63804836.0000 - val_loss: 1366788.1250
Epoch 30/36

2291/2291 [==============================] - 4s 2ms/step - loss: 614750.5625 - val_loss: 240140.6562
Epoch 31/36

2291/2291 [==============================] - 4s 2ms/step - loss: 194849.4688 - val_loss: 58855.5742
Epoch 32/36
2291/2291 [==============================] - 4s 2ms/step - loss: 72744.6797 - val_loss: 39944.3984
Epoch 33/36


2291/2291 [==============================] - 4s 2ms/step - loss: 89909600.0000 - val_loss: 830784.8125
Epoch 34/36
2291/2291 [==============================] - 4s 2ms/step - loss: 425253.6562 - val_loss: 150793.7500
Epoch 35/36

2291/2291 [==============================] - 4s 2ms/step - loss: 165447.2188 - val_loss: 79176.8203
Epoch 36/36


2291/2291 [==============================] - 4s 2ms/step - loss: 78640056.0000 - val_loss: 741879.3750