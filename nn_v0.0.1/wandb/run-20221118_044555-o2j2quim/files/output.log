2022-11-18 04:45:59.605961: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
[34m[1mwandb[39m[22m: [33mWARNING[39m The save_model argument by default saves the model in the HDF5 format that cannot save custom objects like subclassed models and custom layers. This behavior will be deprecated in a future release in favor of the SavedModel format. Meanwhile, the HDF5 model is saved as W&B files and the SavedModel as W&B Artifacts.
Epoch 1/41

1505/1527 [============================>.] - ETA: 0s - loss: 173.9023

1505/1527 [============================>.] - ETA: 0s - loss: 173.9023INFO:tensorflow:Assets written to: /home/oscar47/Desktop/astro101/Astro101-Final-Project/nn_v0.0.1/wandb/run-20221118_044555-o2j2quim/files/model-best/assets
[34m[1mwandb[39m[22m: Adding directory to artifact (/home/oscar47/Desktop/astro101/Astro101-Final-Project/nn_v0.0.1/wandb/run-20221118_044555-o2j2quim/files/model-best)... Done. 0.0s
1527/1527 [==============================] - 4s 2ms/step - loss: 171.4386 - val_loss: 0.8265
Epoch 2/41
1526/1527 [============================>.] - ETA: 0s - loss: 0.8970INFO:tensorflow:Assets written to: /home/oscar47/Desktop/astro101/Astro101-Final-Project/nn_v0.0.1/wandb/run-20221118_044555-o2j2quim/files/model-best/assets
1527/1527 [==============================] - 3s 2ms/step - loss: 0.8969 - val_loss: 0.8093
Epoch 3/41
1527/1527 [==============================] - 3s 2ms/step - loss: 16084233.0000 - val_loss: 135869440.0000
Epoch 4/41
1527/1527 [==============================] - 3s 2ms/step - loss: 783053.6875 - val_loss: 66184.6797
Epoch 5/41
1527/1527 [==============================] - 3s 2ms/step - loss: 35002.7734 - val_loss: 9758.3955
Epoch 6/41

1527/1527 [==============================] - 3s 2ms/step - loss: 14502.5322 - val_loss: 6026.7856
Epoch 7/41
1527/1527 [==============================] - 3s 2ms/step - loss: 7930953.5000 - val_loss: 39818.8125
Epoch 8/41
1527/1527 [==============================] - 3s 2ms/step - loss: 34942.1758 - val_loss: 9394.0322
Epoch 9/41

1527/1527 [==============================] - 3s 2ms/step - loss: 13231.7080 - val_loss: 5335.3818
Epoch 10/41
1527/1527 [==============================] - 3s 2ms/step - loss: 6084.6099 - val_loss: 2376.6833
Epoch 11/41
1527/1527 [==============================] - 3s 2ms/step - loss: 2899.0608 - val_loss: 1457.4109
Epoch 12/41

1527/1527 [==============================] - 3s 2ms/step - loss: 1283.4584 - val_loss: 548.0749
Epoch 13/41
1527/1527 [==============================] - 3s 2ms/step - loss: 19427192.0000 - val_loss: 69436.2969
Epoch 14/41
1527/1527 [==============================] - 3s 2ms/step - loss: 63726.5859 - val_loss: 30404.8281
Epoch 15/41

1527/1527 [==============================] - 3s 2ms/step - loss: 30872.0098 - val_loss: 10119.2158
Epoch 16/41
1527/1527 [==============================] - 3s 2ms/step - loss: 14572.3984 - val_loss: 6774.0933
Epoch 17/41
1527/1527 [==============================] - 3s 2ms/step - loss: 48635716.0000 - val_loss: 467995.9688
Epoch 18/41

1527/1527 [==============================] - 3s 2ms/step - loss: 231979.3750 - val_loss: 241044.6250
Epoch 19/41
1527/1527 [==============================] - 3s 2ms/step - loss: 89311.6641 - val_loss: 32465.7148
Epoch 20/41
1527/1527 [==============================] - 3s 2ms/step - loss: 14795392.0000 - val_loss: 1732714.0000
Epoch 21/41

1527/1527 [==============================] - 3s 2ms/step - loss: 283173.1250 - val_loss: 76046.6875
Epoch 22/41
1527/1527 [==============================] - 3s 2ms/step - loss: 56149.9727 - val_loss: 25516.9180
Epoch 23/41

1527/1527 [==============================] - 3s 2ms/step - loss: 26178.3066 - val_loss: 11322.8867
Epoch 24/41
1527/1527 [==============================] - 3s 2ms/step - loss: 11173.5156 - val_loss: 3789.2888
Epoch 25/41
1527/1527 [==============================] - 2s 2ms/step - loss: 5551.2246 - val_loss: 3556.8782
Epoch 26/41
1527/1527 [==============================] - 3s 2ms/step - loss: 8711004.0000 - val_loss: 45265.4727
Epoch 27/41

1527/1527 [==============================] - 3s 2ms/step - loss: 42362.7812 - val_loss: 14093.4883
Epoch 28/41
1527/1527 [==============================] - 3s 2ms/step - loss: 19174.4551 - val_loss: 8411.4697
Epoch 29/41
1527/1527 [==============================] - 2s 2ms/step - loss: 9252.5664 - val_loss: 4619.8955
Epoch 30/41

1527/1527 [==============================] - 3s 2ms/step - loss: 5343.7305 - val_loss: 2110.0767
Epoch 31/41
1527/1527 [==============================] - 3s 2ms/step - loss: 15241949.0000 - val_loss: 74798.5156
Epoch 32/41
1527/1527 [==============================] - 3s 2ms/step - loss: 107525.6953 - val_loss: 31945.2539
Epoch 33/41

1527/1527 [==============================] - 3s 2ms/step - loss: 26585.8652 - val_loss: 15103.7568
Epoch 34/41
1527/1527 [==============================] - 3s 2ms/step - loss: 16067.7705 - val_loss: 6451.0566
Epoch 35/41
1527/1527 [==============================] - 3s 2ms/step - loss: 40771124.0000 - val_loss: 164989.6250
Epoch 36/41

1527/1527 [==============================] - 3s 2ms/step - loss: 194429.9531 - val_loss: 60583.2148
Epoch 37/41
1527/1527 [==============================] - 3s 2ms/step - loss: 82012.8672 - val_loss: 24562.4902
Epoch 38/41
1527/1527 [==============================] - 3s 2ms/step - loss: 38275.5977 - val_loss: 14728.5850
Epoch 39/41

1527/1527 [==============================] - 3s 2ms/step - loss: 23088.7305 - val_loss: 8583.3506
Epoch 40/41
1527/1527 [==============================] - 3s 2ms/step - loss: 9602858.0000 - val_loss: 65518.2344
Epoch 41/41

1527/1527 [==============================] - 3s 2ms/step - loss: 58108.7578 - val_loss: 25300.1855